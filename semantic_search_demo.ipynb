{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search Engine Demo\n",
    "\n",
    "This notebook demonstrates how to use semantic embeddings to perform semantic search. Semantic search understands the *meaning* of text, not just keyword matching.\n",
    "\n",
    "## What You'll Learn\n",
    "- How to use pre-trained embedding models\n",
    "- How to convert text into numerical vectors (embeddings)\n",
    "- How to find semantically similar documents using cosine similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries\n",
    "\n",
    "First, we need to install the necessary libraries. Run this cell once to install the dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (5.1.0)\n",
      "Requirement already satisfied: numpy in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (1.7.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from sentence-transformers) (4.44.2)\n",
      "Requirement already satisfied: tqdm in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scipy in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from sentence-transformers) (1.16.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from sentence-transformers) (0.35.0)\n",
      "Requirement already satisfied: Pillow in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.9)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.1.31)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the necessary library\n",
    "%pip install sentence-transformers numpy scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries\n",
    "\n",
    "Import the required libraries for our semantic search implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load the Embedding Model\n",
    "\n",
    "We'll use a pre-trained model from Hugging Face. The `all-MiniLM-L6-v2` model is a lightweight but effective model that converts text into 384-dimensional vectors.\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "**Yes, it's a Transformer Neural Network!** Specifically:\n",
    "- **Type**: Transformer encoder (6 layers)\n",
    "- **Base**: Distilled from BERT (Bidirectional Encoder Representations from Transformers)\n",
    "- **Purpose**: Optimized for sentence-level embeddings\n",
    "- **Size**: ~80MB, produces 384-dimensional vectors\n",
    "\n",
    "Transformers use **self-attention mechanisms** to understand relationships between words in a sentence, allowing them to capture semantic meaning effectively.\n",
    "\n",
    "### What This Model Can and Cannot Do\n",
    "\n",
    "**✅ What it CAN do:**\n",
    "- Create embeddings (convert text to numerical vectors)\n",
    "- Find semantically similar documents/texts\n",
    "- Calculate similarity scores between texts\n",
    "- Perform semantic search and retrieval\n",
    "\n",
    "**❌ What it CANNOT do:**\n",
    "- Generate text or chat (it's not a generative model)\n",
    "- Answer questions with generated responses\n",
    "- Create new content\n",
    "- Have conversations\n",
    "\n",
    "**Key Distinction:** This is an **embedding model**, not a **generative/chat model**. It finds the closest matching embeddings from your corpus but doesn't generate new text. For chat capabilities, you'd need models like GPT, Llama, or Claude.\n",
    "\n",
    "**Important Notes:**\n",
    "- **Local Execution**: The model runs **locally** on your machine (not via remote API). All processing happens on your CPU/GPU.\n",
    "- **First Download**: The first time you run this, it will download the model (about 80MB) from Hugging Face Hub and cache it locally.\n",
    "- **Offline Capable**: Subsequent runs will use the cached version - no internet connection needed after the initial download!\n",
    "- **Privacy**: Since everything runs locally, your data never leaves your machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catiefuentes/.pyenv/versions/3.11.7/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load the Embedding Model ---\n",
    "# This loads the model from the Hugging Face Hub (sentence-transformers/all-MiniLM-L6-v2)\n",
    "print(\"Loading model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define the Knowledge Base (Corpus)\n",
    "\n",
    "This is our collection of documents that we want to search through. In a real application, this could be thousands or millions of documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Define the Knowledge Base (Corpus) ---\n",
    "documents = [\n",
    "    \"The sky is a vivid shade of blue today.\",\n",
    "    \"The newest iPhone model was released with a powerful new chip.\",\n",
    "    \"A majestic hawk was spotted flying high above the forest canopy.\",\n",
    "    \"Apple is set to announce its latest mobile device with updated features.\",\n",
    "    \"I'm enjoying a picnic on the grass.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Embeddings for the Corpus\n",
    "\n",
    "Convert each document in our corpus into a numerical vector (embedding). These embeddings capture the semantic meaning of the text.\n",
    "\n",
    "### How Embeddings Work\n",
    "\n",
    "**One-to-One Mapping:** Each chunk of text (sentence/document) gets converted into **one vector**.\n",
    "- 5 sentences → 5 vectors\n",
    "- Each vector has 384 dimensions (for this model)\n",
    "- The entire semantic meaning of the text is encoded into that single vector\n",
    "\n",
    "**Key Concept:** Similar meanings will have similar vectors, even if they use different words!\n",
    "\n",
    "**Example:**\n",
    "- Sentence: \"The newest iPhone model was released...\"\n",
    "- → Single vector: `[0.23, -0.45, 0.12, ..., 0.67]` (384 numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5 embeddings, each with a dimension of 384.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Create Embeddings for the Corpus ---\n",
    "# The .encode() function converts the text into numerical vectors (embeddings)\n",
    "document_embeddings = model.encode(documents, convert_to_tensor=True)\n",
    "print(f\"Generated {len(document_embeddings)} embeddings, each with a dimension of {document_embeddings.shape[1]}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Embedding Structure\n",
    "\n",
    "Let's examine the shape and structure of our embeddings to understand the one-to-one mapping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of document_embeddings: torch.Size([5, 384])\n",
      "\n",
      "This means:\n",
      "  - 5 documents → 5 vectors\n",
      "  - Each vector has 384 dimensions\n",
      "\n",
      "Example: First document embedding (first 10 values):\n",
      "[ 0.02667101  0.02384197  0.08926792  0.03606723  0.02770695  0.00857649\n",
      "  0.06938126 -0.07234967  0.02515115  0.01264223]\n"
     ]
    }
   ],
   "source": [
    "# Examine the embedding structure\n",
    "print(f\"Shape of document_embeddings: {document_embeddings.shape}\")\n",
    "print(f\"\\nThis means:\")\n",
    "print(f\"  - {document_embeddings.shape[0]} documents → {document_embeddings.shape[0]} vectors\")\n",
    "print(f\"  - Each vector has {document_embeddings.shape[1]} dimensions\")\n",
    "print(f\"\\nExample: First document embedding (first 10 values):\")\n",
    "print(document_embeddings[0][:10].cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Define a Query and Create its Embedding\n",
    "\n",
    "Now we'll create a search query. Notice that our query doesn't use the exact same words as the documents, but it should still find the relevant document about phones!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Define a Query and Create its Embedding ---\n",
    "query = \"Tell me about the recent phone technology releases.\"\n",
    "query_embedding = model.encode([query], convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Perform Semantic Search (Calculate Similarity)\n",
    "\n",
    "We calculate the cosine similarity between the query embedding and all document embeddings.\n",
    "\n",
    "**Cosine Similarity:**\n",
    "- Ranges from -1 (opposite meaning) to 1 (identical meaning)\n",
    "- Values close to 1 indicate high semantic similarity\n",
    "- Values close to 0 indicate low similarity\n",
    "\n",
    "We'll find the document with the highest similarity score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Perform Semantic Search (Calculate Similarity) ---\n",
    "# We calculate the cosine similarity between the query embedding and ALL document embeddings.\n",
    "# Cosine similarity ranges from -1 (opposite meaning) to 1 (identical meaning).\n",
    "similarities = cosine_similarity(query_embedding.cpu().numpy(), document_embeddings.cpu().numpy())\n",
    "\n",
    "# Get the index of the most similar document\n",
    "most_similar_index = np.argmax(similarities)\n",
    "max_similarity_score = similarities[0, most_similar_index]\n",
    "best_match_document = documents[most_similar_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Display Results\n",
    "\n",
    "Let's see which document was found as the best match for our query!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Query: **Tell me about the recent phone technology releases.**\n",
      "==================================================\n",
      "Best Match (Score: 0.5846):\n",
      "'Apple is set to announce its latest mobile device with updated features.'\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Print Results ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Query: **{query}**\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best Match (Score: {max_similarity_score:.4f}):\")\n",
    "print(f\"'{best_match_document}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Embedding Dimensions on Hugging Face\n",
    "\n",
    "When exploring models on Hugging Face, you can find the embedding dimension in several ways:\n",
    "\n",
    "### 1. Model Card\n",
    "- Look at the model's main page description\n",
    "- Check the \"Model Card\" section for specifications\n",
    "- Often listed as \"embedding dimension\", \"hidden size\", or \"model dimension\"\n",
    "\n",
    "### 2. Config File\n",
    "- Go to the \"Files and versions\" tab\n",
    "- Open `config.json`\n",
    "- Look for parameters like:\n",
    "  - `hidden_size` - internal representation size\n",
    "  - `embedding_size` - output embedding dimension\n",
    "  - `d_model` - model dimension (common in transformers)\n",
    "\n",
    "### 3. Model Usage Example\n",
    "- Check code examples in the model card\n",
    "- Often shows the output shape/dimension\n",
    "\n",
    "**Example:** For `all-MiniLM-L6-v2`, the config shows `hidden_size: 384`, which is the embedding dimension.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Dimensions: Embedding Models vs. Chatbot Models\n",
    "\n",
    "**Important Distinction:** There are two types of models with different purposes:\n",
    "\n",
    "### Embedding Models (for Semantic Search)\n",
    "These produce **output embeddings** for similarity search:\n",
    "\n",
    "| Model | Embedding Dimension | Use Case |\n",
    "|-------|-------------------|----------|\n",
    "| `all-MiniLM-L6-v2` | **384** | Lightweight semantic search |\n",
    "| `all-mpnet-base-v2` | **768** | Higher quality embeddings |\n",
    "| `BGE-base-en-v1.5` | **768** | General-purpose embeddings |\n",
    "| `e5-large-v2` | **1024** | High-quality multilingual |\n",
    "\n",
    "### Chatbot Models (Generative LLMs)\n",
    "These have **internal/hidden dimensions** but don't produce embeddings for search:\n",
    "\n",
    "| Model | Hidden Dimension | Type | Note |\n",
    "|-------|-----------------|------|-----|\n",
    "| **GPT-3.5** | ~4096 | Generative | Internal representation, not for embeddings |\n",
    "| **GPT-4** | ~8192+ | Generative | Not publicly disclosed, estimated |\n",
    "| **Llama 3.1 8B** | **4096** | Generative | Internal hidden size |\n",
    "| **Llama 3.1 70B** | **8192** | Generative | Internal hidden size |\n",
    "| **Claude 3** | ~4096+ | Generative | Not publicly disclosed |\n",
    "| **BERT-base** | **768** | Encoder | Can be used for embeddings |\n",
    "\n",
    "**Key Point:** Chatbot models are **generative** (create text), while embedding models are **retrieval-focused** (find similar text). They serve different purposes!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Explore All Similarity Scores (Optional)\n",
    "\n",
    "Let's see the similarity scores for all documents to better understand how the semantic search works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity scores for all documents:\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 1 (Score: 0.0485):\n",
      "  'The sky is a vivid shade of blue today.'\n",
      "\n",
      "Document 2 (Score: 0.5450):\n",
      "  'The newest iPhone model was released with a powerful new chip.'\n",
      "\n",
      "Document 3 (Score: -0.0547):\n",
      "  'A majestic hawk was spotted flying high above the forest canopy.'\n",
      "\n",
      "Document 4 (Score: 0.5846):\n",
      "  'Apple is set to announce its latest mobile device with updated features.'\n",
      "\n",
      "Document 5 (Score: -0.0379):\n",
      "  'I'm enjoying a picnic on the grass.'\n"
     ]
    }
   ],
   "source": [
    "# Display similarity scores for all documents\n",
    "print(\"\\nSimilarity scores for all documents:\")\n",
    "print(\"-\" * 50)\n",
    "for i, (doc, score) in enumerate(zip(documents, similarities[0])):\n",
    "    print(f\"\\nDocument {i+1} (Score: {score:.4f}):\")\n",
    "    print(f\"  '{doc}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try It Yourself!\n",
    "\n",
    "Experiment with different queries to see how semantic search works:\n",
    "\n",
    "- Try queries about nature, technology, or daily activities\n",
    "- Notice how the model finds relevant documents even when they don't share exact keywords\n",
    "- Compare the similarity scores to understand the ranking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Semantic Search to RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "**You're absolutely correct!** Combining an embedding model with a chat model creates a **RAG (Retrieval-Augmented Generation)** system. This is one of the most powerful applications of semantic embeddings!\n",
    "\n",
    "### How RAG Works\n",
    "\n",
    "**Step 1: Retrieval (Embedding Model)**\n",
    "- User asks a question: *\"What are the latest iPhone features?\"*\n",
    "- Embedding model searches your knowledge base (documents, articles, etc.)\n",
    "- Finds the most relevant text chunks using semantic similarity\n",
    "\n",
    "**Step 2: Augmentation (Context Injection)**\n",
    "- The retrieved relevant documents are passed as context to the chat model\n",
    "- This gives the chat model specific, relevant information to work with\n",
    "\n",
    "**Step 3: Generation (Chat Model)**\n",
    "- The chat model (GPT, Claude, Llama, etc.) generates an answer\n",
    "- It uses the retrieved context + its training knowledge\n",
    "- Produces a well-informed, contextually relevant response\n",
    "\n",
    "### Why RAG is Powerful\n",
    "\n",
    "✅ **Reduces Hallucinations**: Chat models can make up information. RAG grounds answers in real documents.\n",
    "\n",
    "✅ **Domain-Specific Knowledge**: You can use your own documents/knowledge base without retraining the model.\n",
    "\n",
    "✅ **Up-to-Date Information**: Add new documents without retraining - just update your knowledge base.\n",
    "\n",
    "✅ **Transparency**: You can see which documents were used to generate the answer (citations).\n",
    "\n",
    "### Example RAG Pipeline\n",
    "\n",
    "```\n",
    "User Question\n",
    "    ↓\n",
    "[Embedding Model] → Finds relevant documents from knowledge base\n",
    "    ↓\n",
    "[Retrieved Context] → \"iPhone 15 Pro features: A17 chip, titanium design...\"\n",
    "    ↓\n",
    "[Chat Model] → Generates answer using context\n",
    "    ↓\n",
    "Final Answer: \"Based on the latest information, the iPhone 15 Pro features...\"\n",
    "```\n",
    "\n",
    "**This notebook demonstrates Step 1 (Retrieval).** To build a full RAG system, you'd add a chat model for Step 3!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Query: **do people see hawks under the rain?**\n",
      "==================================================\n",
      "Best Match (Score: 0.4604):\n",
      "'A majestic hawk was spotted flying high above the forest canopy.'\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Define a Query and Create its Embedding ---\n",
    "query = \"do people see hawks under the rain?\"\n",
    "query_embedding = model.encode([query], convert_to_tensor=True)\n",
    "# --- 5. Perform Semantic Search (Calculate Similarity) ---\n",
    "# We calculate the cosine similarity between the query embedding and ALL document embeddings.\n",
    "# Cosine similarity ranges from -1 (opposite meaning) to 1 (identical meaning).\n",
    "similarities = cosine_similarity(query_embedding.cpu().numpy(), document_embeddings.cpu().numpy())\n",
    "\n",
    "# Get the index of the most similar document\n",
    "most_similar_index = np.argmax(similarities)\n",
    "max_similarity_score = similarities[0, most_similar_index]\n",
    "best_match_document = documents[most_similar_index]\n",
    "# --- 6. Print Results ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Query: **{query}**\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best Match (Score: {max_similarity_score:.4f}):\")\n",
    "print(f\"'{best_match_document}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
